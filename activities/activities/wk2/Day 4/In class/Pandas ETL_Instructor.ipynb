{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273e3fa7-e061-4000-9999-ed75af7178e2",
   "metadata": {},
   "source": [
    "# Data Engineering Project Instructions\n",
    "\n",
    "## Preparation\n",
    "* You will need to install `boto3`\n",
    "* You will be using `pandas` and `configparser`\n",
    "* You will need to install two additional libraries to support pandas in reading from S3:\n",
    "```bash\n",
    "  conda install s3fs -c conda-forge -y\n",
    "  conda install fsspec -y\n",
    "```\n",
    "\n",
    "## Transformations\n",
    "\n",
    "You will perform the same transformations you did in the previous class, including:\n",
    "\n",
    "* __Calculate Trip Duration in Minutes:__ Call it Trip_Duration.\n",
    "* __Calculate Total Trip Charge:__ Include fare amount, extra, MTA tax, tolls amount, improvement surcharge, congestion surcharge, airport fee, and tip amount. Call it Total_Trip_Charge.\n",
    "* __Add Trip Date Components:__ Add `Trip_Date`, `Trip_Month`, `Trip_Day`, and `Trip_Year`.\n",
    "* __Keep Specific Columns:__ `VendorID`, `passenger_count`, `trip_distance`, `store_and_fwd_flag`, `payment_type`, `Trip_Duration`, `Total_Trip_Charge`, `Trip_Date`, `Trip_Month`, `Trip_Year`, `Trip_Day`\n",
    "* __Reorder the Columns:__ Start with VendorID, followed by all the date/time columns (Date, Year, Month, and Day), then the remaining columns.\n",
    "* __Rename the Columns:__ Rename columns to Vendor_ID, No_of_Passengers, SF_Flag, Payment_Type.\n",
    "\n",
    "## Expectation Part 1\n",
    "\n",
    "* You will create functions to support your ETL Process.\n",
    "* You will need to capture some statistics for each data set you are processing for reference.\n",
    "* You will read from a source S3 bucket (raw) and write into a different S3 bucket (transformed):\n",
    "```python\n",
    "bucket_dest = 'techcatalyst-transformed'\n",
    "bucket_source = 'techcatalyst-raw'\n",
    "```\n",
    "* Make sure that your files \"objects\" are under your name inside the `techcatalyst-transformed` bucket. For example:\n",
    "s3://techcatalyst-transformed/tarek/yellow_tripdata_2024-01_transformed.parquet/Trip_Year=2024/Trip_Month=January/84e2f047dcff4f7183ae25518ecd486b-0.parquet\n",
    "* Create a function that generates an `s3://` URI. The function should take a bucket name, a file name, and then construct an `s3://` URI that points to the object. Refer to this link for more details on S3 URIs. If you forget how the `s3://` URI looks, please log in to AWS and navigate to S3 services to see how an object inside a bucket is being referenced by the `s3://` URI.\n",
    "* Create a **cleanup** function that takes three *parameters*: the DataFrame, the name of the file, and the destination bucket. The function will transform the data, then write out a Parquet file to the destination address.\n",
    "* You may need to create additional functions as necessary.\n",
    "* Finally, once you write the files to S3, you will also write your statistic file. For example., capture number of rows, number of columns, number of columns with null values, date/time of processing. What else can you think of?\n",
    "* While processing each file, it would be great to log something on the screen to show progress or status. Here is an example below:\n",
    "\n",
    "```\n",
    "processing yellow_tripdata_2024-01.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-02.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-02.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-03.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-03.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-04.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-04.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Expectation Part 2\n",
    "* Once the data has been loaded on AWS, you will need to use Athena to inspect the data to ensure correctness.\n",
    "* Remember, before using Athena, you will need to go through the process of creating the crawlers and other setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fe52dd-1418-4531-8907-152ff454fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbae517-ae6a-43d2-bbb5-ee350a13fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# read the cfg file\n",
    "config.read('aws.cfg')\n",
    "\n",
    "AWS_ACCESS_KEY = config['AWS']['aws_access_key']\n",
    "AWS_SECRET_KEY = config['AWS']['aws_secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89f9ed3-3b3a-4a76-9c3c-3e0a14505526",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', \n",
    "                             aws_access_key_id = AWS_ACCESS_KEY,\n",
    "                             aws_secret_access_key = AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2509e16-db47-43f6-9dd6-11aedb91cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ea410c-09ed-4e34-ba8a-403956f5556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [bucket['Name'] for bucket in response['Buckets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d7c951-4790-4474-aeb6-239415746401",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'techcatalyst-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb423ed0-64c6-40c5-9692-e54107419925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ResponseMetadata', 'IsTruncated', 'Contents', 'Name', 'Prefix', 'MaxKeys', 'EncodingType', 'KeyCount'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_objs = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "response_objs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac155fe-6ab5-4c25-900e-8bd340c3f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2024-01.parquet\n",
      "yellow_tripdata_2024-02.parquet\n",
      "yellow_tripdata_2024-03.parquet\n",
      "yellow_tripdata_2024-04.parquet\n"
     ]
    }
   ],
   "source": [
    "for obj in response_objs['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90336f7-5d4e-46b0-aaa5-8b0f5d47cd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_tripdata_2024-01.parquet',\n",
       " 'yellow_tripdata_2024-02.parquet',\n",
       " 'yellow_tripdata_2024-03.parquet',\n",
       " 'yellow_tripdata_2024-04.parquet']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [obj['Key'] for obj in response_objs['Contents']]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28113b-5c6b-4505-865d-287729ee2647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85546ae-4420-4a82-8ef2-bc95213b9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_urls = []\n",
    "for file in files:\n",
    "    url = f\"s3://{bucket_name}/{file}\"\n",
    "    s3_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c383f0ab-f89f-460d-8354-c701ea46e1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet',\n",
       " 's3://techcatalyst-raw/yellow_tripdata_2024-02.parquet',\n",
       " 's3://techcatalyst-raw/yellow_tripdata_2024-03.parquet',\n",
       " 's3://techcatalyst-raw/yellow_tripdata_2024-04.parquet']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152b35f3-4ead-4245-897f-1377eeb8f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/yellow_tripdata_2024-01', 'parquet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_urls[0].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2710294f-52ff-4631-b568-dc353643ef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://techcatalyst-raw/yellow_tripdata_2024-01_transformed'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_urls[0].split('.')[0] + '_transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "774f4959-64c6-4c8e-b3df-fd71f7a3a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87fd7c-e183-4020-9a3d-a109d6a211b6",
   "metadata": {},
   "source": [
    "if you get this error, it will indicate which library is missing as shown:\n",
    "```\n",
    "ImportError: Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e4bfe1-97f7-4dea-9938-15547dddd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install fsspec -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4daf499-8908-4515-bc49-c5dce4f8a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install s3fs -c conda-forge -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "982bfe30-d956-4c1e-b246-cd220ee03378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_parquet('s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet',\n",
    "#                storage_options={\n",
    "#                    'key' : AWS_ACCESS_KEY,\n",
    "#                    'secret' : AWS_SECRET_KEY\n",
    "#                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5290a-e606-4aca-808f-8a3a1e2a3167",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69cf040a-27dd-4537-925c-3238af0c2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that generates an s3:// URI \n",
    "# The function takes a bucket name, a file name and then constructs an s3:// uri that points to the object  \n",
    "\n",
    "# https://repost.aws/questions/QUFXlwQxxJQQyg9PMn2b6nTg/what-is-s3-uri-in-simple-storage-service\n",
    "\n",
    "# if you forgot how the s3:// uri looks like then please login to AWS and navigate to S3 services and see how an object inside a bucket is being referenced by the s3:// uri\n",
    "\n",
    "def generate_url(bucket, file):\n",
    "        url = f\"s3://{bucket_name}/{file}\"\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaca5291-e03d-4dd9-a8bf-c20493f8d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleanup function that takes three parameters: the DataFrame, the name of the file, the destination bucket\n",
    "\n",
    "# the function will transform the data then write out a Parquet file to the destination address \n",
    "from datetime import datetime\n",
    "\n",
    "def cleanup(df, name, dest):\n",
    "\n",
    "    # calculate Trip Duration in Minutes \n",
    "    print(f'processing {name}')\n",
    "    print('...........')\n",
    "    df['Trip_Duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() // 60\n",
    "    \n",
    "    # Calculate Total Trip Charge to inclide: fare amount, extra, mta_tax, tolls amount, improvement surcharge, congestion surcharge, airport fee, and tip amount\n",
    "    df['Total_Trip_Charge'] = df['fare_amount'] + df['extra'] + df['mta_tax'] + df['tolls_amount'] + df['improvement_surcharge'] + df['congestion_surcharge'] + df['Airport_fee'] + df['tip_amount']\n",
    "    \n",
    "    # Add Trip Date, Trip Month, Trip Day, and Trip Year\n",
    "    \n",
    "    df['Trip_Date'] = pd.to_datetime(df['tpep_pickup_datetime'].dt.date)\n",
    "    df['Trip_Month'] = df['Trip_Date'].dt.month_name()\n",
    "    df['Trip_Day'] = df['Trip_Date'].dt.day_name()\n",
    "    df['Trip_Year'] = df['Trip_Date'].dt.year\n",
    "    \n",
    "    \n",
    "    cols = ['VendorID', 'Trip_Date', 'Trip_Year', 'Trip_Month', 'Trip_Day',\n",
    "           'passenger_count', 'trip_distance', 'store_and_fwd_flag',\n",
    "           'payment_type', 'Trip_Duration',\n",
    "           'Total_Trip_Charge' ]\n",
    "    \n",
    "    df = df[cols]\n",
    "    df = df.rename(columns={\n",
    "        'VendorID': 'Vendor_ID',\n",
    "        'passenger_count': 'No_of_Passengers',\n",
    "        'store_and_fwd_flag': 'SF_Flag',\n",
    "        'payment_type': 'Payment_Type'\n",
    "    })\n",
    "    \n",
    "    print(f'writing {obj} to {dest} bucket')\n",
    "    print('...........')\n",
    "\n",
    "    number_of_records = df.shape[0]\n",
    "    number_of_columns = df.shape[1]\n",
    "    number_of_columns_with_na = (df.isnull().sum() > 0).sum().tolist()\n",
    "    \n",
    "    \n",
    "\n",
    "    stats = {\n",
    "        'file_name': [name],\n",
    "        'number_of_records': [number_of_records],\n",
    "        'number_of_cols': [number_of_columns],\n",
    "        'number_of_cols_with_na': [number_of_columns_with_na],\n",
    "        'date_time':  f'{datetime.now()}'\n",
    "    }\n",
    "\n",
    "    # example for creating the URI or better yet by utilizing the generate_url \n",
    "    file = f\"s3://{dest}/tarek/{name.split('.')[0]}_transformed.parquet\"\n",
    "    \n",
    "\n",
    "    df.to_parquet(file, partition_cols=['Trip_Year', 'Trip_Month'],\n",
    "                  storage_options={\n",
    "                   'key' : AWS_ACCESS_KEY,\n",
    "                   'secret' : AWS_SECRET_KEY\n",
    "               })\n",
    "\n",
    "\n",
    "    return pd.DataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c4bb2e8-91a1-4f72-a62f-72fa9ed0a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing yellow_tripdata_2024-01.parquet\n",
      "...........\n",
      "writing s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet to techcatalyst-transformed bucket\n",
      "...........\n",
      "CPU times: user 3.58 s, sys: 787 ms, total: 4.37 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_dest = 'techcatalyst-transformed'\n",
    "bucket_source = 'techcatalyst-raw'\n",
    "\n",
    "\n",
    "stats_df = pd.DataFrame(columns=['file_name', 'number_of_records', 'number_of_cols', 'number_of_cols_with_na', 'date_time'])\n",
    "\n",
    "# Example running for first file for testing \n",
    "for file in files[0:1]:\n",
    "    obj = generate_url(bucket_source, file)\n",
    "    df = pd.read_parquet(obj,\n",
    "               storage_options={\n",
    "                   'key' : AWS_ACCESS_KEY,\n",
    "                   'secret' : AWS_SECRET_KEY\n",
    "               })\n",
    "    stats_returned = cleanup(df, file, bucket_dest)\n",
    "    stats_df = pd.concat([stats_df, stats_returned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c93ffd-9eb5-4273-b721-fee1219fbcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>number_of_records</th>\n",
       "      <th>number_of_cols</th>\n",
       "      <th>number_of_cols_with_na</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow_tripdata_2024-01.parquet</td>\n",
       "      <td>2964624</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-27 18:56:04.516944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name number_of_records number_of_cols  \\\n",
       "0  yellow_tripdata_2024-01.parquet           2964624             11   \n",
       "\n",
       "  number_of_cols_with_na                   date_time  \n",
       "0                      3  2024-06-27 18:56:04.516944  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev2]",
   "language": "python",
   "name": "conda-env-dev2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
